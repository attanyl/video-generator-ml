{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_diffusion_pytorch import GaussianDiffusion\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import gc\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from einops import rearrange\n",
    "from einops_exts import check_shape, rearrange_many\n",
    "import math\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def is_odd(n):\n",
    "    return (n % 2) == 1\n",
    "\n",
    "def Upsample(dim):\n",
    "    return nn.ConvTranspose3d(dim, dim, (1, 4, 4), (1, 2, 2), (0, 1, 1))\n",
    "\n",
    "def Downsample(dim):\n",
    "    return nn.Conv3d(dim, dim, (1, 4, 4), (1, 2, 2), (0, 1, 1))\n",
    "\n",
    "def prob_mask_like(shape, prob, device):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "    else:\n",
    "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinopsToAndFrom(nn.Module):\n",
    "    def __init__(self, from_einops, to_einops, fn):\n",
    "        super().__init__()\n",
    "        self.from_einops = from_einops\n",
    "        self.to_einops = to_einops\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        shape = x.shape\n",
    "        reconstitute_kwargs = dict(tuple(zip(self.from_einops.split(' '), shape)))\n",
    "        x = rearrange(x, f'{self.from_einops} -> {self.to_einops}')\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x = rearrange(x, f'{self.to_einops} -> {self.from_einops}', **reconstitute_kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        rotary_emb = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.to_qkv = nn.Linear(dim, hidden_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(hidden_dim, dim, bias = False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        pos_bias = None,\n",
    "        focus_present_mask = None\n",
    "    ):\n",
    "        n, device = x.shape[-2], x.device\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "\n",
    "        if exists(focus_present_mask) and focus_present_mask.all():\n",
    "            # if all batch samples are focusing on present\n",
    "            # it would be equivalent to passing that token's values through to the output\n",
    "            values = qkv[-1]\n",
    "            return self.to_out(values)\n",
    "\n",
    "        # split out heads\n",
    "\n",
    "        q, k, v = rearrange_many(qkv, '... n (h d) -> ... h n d', h = self.heads)\n",
    "\n",
    "        # scale\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # rotate positions into queries and keys for time attention\n",
    "\n",
    "        if exists(self.rotary_emb):\n",
    "            q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "            k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "        # similarity\n",
    "\n",
    "        sim = einsum('... h i d, ... h j d -> ... h i j', q, k)\n",
    "\n",
    "        # relative positional bias\n",
    "\n",
    "        if exists(pos_bias):\n",
    "            sim = sim + pos_bias\n",
    "\n",
    "        if exists(focus_present_mask) and not (~focus_present_mask).all():\n",
    "            attend_all_mask = torch.ones((n, n), device = device, dtype = torch.bool)\n",
    "            attend_self_mask = torch.eye(n, device = device, dtype = torch.bool)\n",
    "\n",
    "            mask = torch.where(\n",
    "                rearrange(focus_present_mask, 'b -> b 1 1 1 1'),\n",
    "                rearrange(attend_self_mask, 'i j -> 1 1 1 i j'),\n",
    "                rearrange(attend_all_mask, 'i j -> 1 1 1 i j'),\n",
    "            )\n",
    "\n",
    "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        # numerical stability\n",
    "\n",
    "        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = einsum('... h i j, ... h j d -> ... h i d', attn, v)\n",
    "        out = rearrange(out, '... h n d -> ... n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        heads = 8,\n",
    "        num_buckets = 32,\n",
    "        max_distance = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def _relative_position_bucket(relative_position, num_buckets = 32, max_distance = 128):\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "\n",
    "        num_buckets //= 2\n",
    "        ret += (n < 0).long() * num_buckets\n",
    "        n = torch.abs(n)\n",
    "\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "\n",
    "    def forward(self, n, device):\n",
    "        q_pos = torch.arange(n, dtype = torch.long, device = device)\n",
    "        k_pos = torch.arange(n, dtype = torch.long, device = device)\n",
    "        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n",
    "        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n",
    "        values = self.relative_attention_bias(rp_bucket)\n",
    "        return rearrange(values, 'i j h -> h i j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv3d(dim, dim_out, (1, 3, 3), padding = (0, 1, 1))\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "        self.res_conv = nn.Conv3d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp):\n",
    "            assert exists(time_emb), 'time emb must be passed in'\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialLinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, f, h, w = x.shape\n",
    "        x = rearrange(x, 'b c f h w -> (b f) c h w')\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = rearrange_many(qkv, 'b (h c) x y -> b h c (x y)', h = self.heads)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
    "        out = self.to_out(out)\n",
    "        return rearrange(out, '(b f) c h w -> b c f h w', b = b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.has_cond = True\n",
    "        \n",
    "        dim = 64 # the width and height of the feature map will be 64\n",
    "        attn_heads = 8 # the number of heads for the spatial attention\n",
    "        attn_dim_head = 32 # the dimension of each head for the spatial attention\n",
    "\n",
    "        # temporal attention and its relative positional encoding\n",
    "        rotary_emb = RotaryEmbedding(attn_dim_head) # positional encoding for the temporal attention\n",
    "        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', 'b (h w) f c', Attention(dim, heads = attn_heads, dim_head = attn_dim_head, rotary_emb = rotary_emb))\n",
    "\n",
    "        self.time_rel_pos_bias = RelativePositionBias(heads = attn_heads, max_distance = 32)\n",
    "\n",
    "        # initial conv\n",
    "        init_kernel_size = 7\n",
    "        init_padding = init_kernel_size // 2\n",
    "        self.init_conv = nn.Conv3d(3, dim, (1, init_kernel_size, init_kernel_size), padding = (0, init_padding, init_padding))\n",
    "\n",
    "        self.init_temporal_attn = Residual(PreNorm(dim, temporal_attn(dim)))\n",
    "\n",
    "        # dimensions\n",
    "        dims = [dim, *map(lambda m: dim * m, [1, 2, 4, 8])]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time conditioning\n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # text conditioning\n",
    "        self.null_cond_emb = nn.Parameter(torch.randn(1, BERT_MODEL_DIM))\n",
    "        cond_dim = time_dim + BERT_MODEL_DIM\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        # block type\n",
    "        block_klass = partial(ResnetBlock, groups=8)\n",
    "        block_klass_cond = partial(block_klass, time_emb_dim=cond_dim)\n",
    "\n",
    "        # modules for all layers\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass_cond(dim_in, dim_out),\n",
    "                block_klass_cond(dim_out, dim_out),\n",
    "                Residual(PreNorm(dim_out, SpatialLinearAttention(dim_out, heads = attn_heads))),\n",
    "                Residual(PreNorm(dim_out, temporal_attn(dim_out))),\n",
    "                Downsample(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass_cond(mid_dim, mid_dim)\n",
    "\n",
    "        spatial_attn = EinopsToAndFrom('b c f h w', 'b f (h w) c', Attention(mid_dim, heads = attn_heads))\n",
    "\n",
    "        self.mid_spatial_attn = Residual(PreNorm(mid_dim, spatial_attn))\n",
    "        self.mid_temporal_attn = Residual(PreNorm(mid_dim, temporal_attn(mid_dim)))\n",
    "\n",
    "        self.mid_block2 = block_klass_cond(mid_dim, mid_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_klass_cond(dim_out * 2, dim_in),\n",
    "                block_klass_cond(dim_in, dim_in),\n",
    "                Residual(PreNorm(dim_in, SpatialLinearAttention(dim_in, heads = attn_heads))),\n",
    "                Residual(PreNorm(dim_in, temporal_attn(dim_in))),\n",
    "                Upsample(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        out_dim = 3\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim * 2, dim),\n",
    "            nn.Conv3d(dim, out_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        *args,\n",
    "        cond_scale = 2.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        logits = self.forward(*args, null_cond_prob = 0., **kwargs)\n",
    "        if cond_scale == 1:\n",
    "            return logits\n",
    "\n",
    "        null_logits = self.forward(*args, null_cond_prob = 1., **kwargs)\n",
    "        return null_logits + (logits - null_logits) * cond_scale\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        time,\n",
    "        cond,\n",
    "        null_cond_prob = 0.,\n",
    "    ):\n",
    "        batch, device = x.shape[0], x.device\n",
    "\n",
    "        focus_present_mask = (lambda: prob_mask_like((batch,), 0, device = device))()\n",
    "\n",
    "        time_rel_pos_bias = self.time_rel_pos_bias(x.shape[2], device = x.device)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x = self.init_temporal_attn(x, pos_bias = time_rel_pos_bias)\n",
    "\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
    "\n",
    "        # classifier free guidance\n",
    "        if self.has_cond:\n",
    "            batch, device = x.shape[0], x.device\n",
    "            mask = prob_mask_like((batch,), null_cond_prob, device = device)\n",
    "            cond = torch.where(rearrange(mask, 'b -> b 1'), self.null_cond_emb, cond)\n",
    "            t = torch.cat((t, cond), dim = -1)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, spatial_attn, temporal_attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = spatial_attn(x)\n",
    "            x = temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_spatial_attn(x)\n",
    "        x = self.mid_temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, spatial_attn, temporal_attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = spatial_attn(x)\n",
    "            x = temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=64,\n",
    "    num_frames=10,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type='l1',     # L1 or L2\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert = BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the model\n",
    "# diffusion.load_state_dict(torch.load('diffusion.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_7688\\657186772.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  gif = torch.tensor(gif).cuda()\n",
      "Using cache found in C:\\Users\\david/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\david/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0, loss 0.8616583943367004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     gif \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([gif, np\u001b[39m.\u001b[39mrepeat(gif[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:], \u001b[39m10\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(gif), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)])\n\u001b[0;32m     20\u001b[0m gif \u001b[39m=\u001b[39m gif[:\u001b[39m10\u001b[39m]\n\u001b[1;32m---> 21\u001b[0m gif \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(gif)\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     22\u001b[0m gif \u001b[39m=\u001b[39m gif\u001b[39m.\u001b[39mpermute(\u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39m# (channels, frames, height, width)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m gif \u001b[39m=\u001b[39m gif \u001b[39m/\u001b[39m \u001b[39m127.5\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH = 1\n",
    "optim = torch.optim.Adam(diffusion.parameters(), lr=1e-4)\n",
    "\n",
    "# train the diffusion model\n",
    "for i in range(20):\n",
    "    gif_names = os.listdir('gifs_64')\n",
    "    \n",
    "    # loop through the gifs 32 at a time\n",
    "    for j in range(0, len(gif_names), BATCH):\n",
    "        # the conds are the names of the gifs, remove the .gif extension\n",
    "        names = gif_names[j:j+BATCH]\n",
    "        conds = [name.split('.')[0] for name in names]\n",
    "        \n",
    "        # read the gifs and convert to tensors\n",
    "        gifs = []\n",
    "        for name in names:\n",
    "            gif = imageio.mimread('gifs_64/' + name)\n",
    "            if len(gif) < 10:\n",
    "                gif = np.concatenate([gif, np.repeat(gif[-1:], 10 - len(gif), axis=0)])\n",
    "            gif = gif[:10]\n",
    "            gif = torch.tensor(gif).cuda()\n",
    "            gif = gif.permute(3, 0, 1, 2) # (channels, frames, height, width)\n",
    "            gif = gif / 127.5 - 1\n",
    "            gifs.append(gif)\n",
    "        gifs = torch.stack(gifs)\n",
    "        \n",
    "        # zero the gradients\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # get the loss\n",
    "        loss = diffusion(gifs, cond=conds)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights\n",
    "        optim.step()\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(f'epoch {i}, batch {j}, loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(diffusion.state_dict(), 'diffusion.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a baby is playing with an orange toy\n"
     ]
    }
   ],
   "source": [
    "# get the 230'th name from gifs_64\n",
    "best = os.listdir('gifs_64')[230].split('.')[0]\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [1:01:20<00:00,  3.68s/it]   \n"
     ]
    }
   ],
   "source": [
    "txt = 'green'\n",
    "\n",
    "output_gif = diffusion.sample(cond=[txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tensor to a gif that we can see\n",
    "out = output_gif[0].cpu().numpy()\n",
    "out = np.transpose(out, (1, 2, 3, 0))\n",
    "# the output is between -1 and 1, so we need to scale it to 0-255\n",
    "out = ((out + 1) / 2 * 255).astype(np.uint8)\n",
    "imageio.mimsave('out.gif', out, duration = 1000, loop = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
