{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageSequence\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from einops import rearrange\n",
    "from einops_exts import check_shape, rearrange_many\n",
    "import math\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\david/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\david/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BERT_MODEL_DIM = 768\n",
    "TOKENIZER = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "MODEL = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    MODEL = MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, add_special_tokens = True):\n",
    "    if not isinstance(texts, (list, tuple)):\n",
    "        texts = [texts]\n",
    "\n",
    "    encoding = TOKENIZER.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens = add_special_tokens,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    token_ids = encoding.input_ids\n",
    "    return token_ids\n",
    "\n",
    "@torch.no_grad()\n",
    "def bert_embed(\n",
    "    token_ids,\n",
    "    return_cls_repr = False,\n",
    "    eps = 1e-8,\n",
    "    pad_id = 0.\n",
    "):\n",
    "    mask = token_ids != pad_id\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        token_ids = token_ids.cuda()\n",
    "        mask = mask.cuda()\n",
    "\n",
    "    outputs = MODEL(input_ids = token_ids, attention_mask = mask, output_hidden_states = True)\n",
    "    hidden_state = outputs.hidden_states[-1]\n",
    "    cls_repr = outputs[0][:, 0, :] / (outputs[0][:, 0, :].norm(dim = -1, keepdim = True) + eps)\n",
    "\n",
    "    if return_cls_repr:\n",
    "        return hidden_state[:, 0]\n",
    "    \n",
    "    if mask is None:\n",
    "        return hidden_state.mean(dim = 1)\n",
    "    \n",
    "    mask = rearrange(mask[:, 1:], 'b n -> b n 1')\n",
    "\n",
    "    denom = mask.sum(dim = 1)\n",
    "    return (hidden_state[:, 1:] * mask).sum(dim = 1) / (denom + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, d):\n",
    "    if val is not None:\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def Upsample(dim):\n",
    "    return nn.ConvTranspose3d(dim, dim, (1, 4, 4), (1, 2, 2), (0, 1, 1))\n",
    "\n",
    "def Downsample(dim):\n",
    "    return nn.Conv3d(dim, dim, (1, 4, 4), (1, 2, 2), (0, 1, 1))\n",
    "\n",
    "def prob_mask_like(shape, prob, device):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "    else:\n",
    "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n",
    "    \n",
    "def normalize_img(t):\n",
    "    return t * 2 - 1\n",
    "\n",
    "def unnormalize_img(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "def extract(a, t, shape):\n",
    "    b, *_ = t.shape\n",
    "    gathered = a.gather(-1, t)\n",
    "    return gathered.reshape(b, *((1,) * (len(shape) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(torch.nn.Module):\n",
    "    def __init__(self, heads = 8, buckets = 32, max_distance = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.buckets = buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.relative_attention_bias = torch.nn.Embedding(buckets, heads)\n",
    "\n",
    "    def forward(self, n, device):\n",
    "\n",
    "        q_pos = torch.arange(n, dtype = torch.long, device = device)\n",
    "        k_pos = torch.arange(n, dtype = torch.long, device = device)\n",
    "        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n",
    "\n",
    "        rp_bucket = 0\n",
    "        buckets = self.buckets // 2\n",
    "        rp_bucket += (n < 0).long() * buckets\n",
    "        n = torch.abs(-rel_pos)\n",
    "\n",
    "        max_exact = buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(self.max_distance / max_exact) * (buckets - max_exact)\n",
    "        ).long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, buckets - 1))\n",
    "\n",
    "        rp_bucket += torch.where(is_small, n, val_if_large)\n",
    "        return rearrange(self.relative_attention_bias(rp_bucket), 'i j h -> h i j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        dev = x.device\n",
    "        dim = self.dim // 2\n",
    "        emb = torch.exp(torch.arange(dim, device=dev) * -(math.log(10000) / (dim - 1)))\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        return torch.cat((emb.sin(), emb.cos()), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        variance = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        return (x - torch.mean(x, dim = 1, keepdim = True)) / (variance + self.eps).sqrt() * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.layer_norm = LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.layer_norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = torch.nn.Conv3d(dim_in, dim_out, (1, 3, 3), padding = (0, 1, 1))\n",
    "        self.norm = torch.nn.GroupNorm(groups, dim_out)\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(self.norm(x))\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        return self.silu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, *, time = None, groups = 8):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(time, dim_out * 2)\n",
    "        ) if (time is not None) else None\n",
    "\n",
    "        self.block1 = Block(dim_in, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "        self.conv = torch.nn.Conv3d(dim_in, dim_out, 1) if dim_in != dim_out else torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x, time = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if time is not None:\n",
    "            assert time is not None, 'time embedding must be supplied if using time embedding mlp'\n",
    "            time = self.seq(time)\n",
    "            time = rearrange(time, 'b c -> b c 1 1 1')\n",
    "            scale_shift = time.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "        return self.block2(h) + self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialLinearAttention(torch.nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.conv1 = torch.nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, h, w = x.shape\n",
    "        x = rearrange(x, 'b c f h w -> (b f) c h w')\n",
    "\n",
    "        qkv = self.conv1(x).chunk(3, dim = 1)\n",
    "        q, k, v = rearrange_many(qkv, 'b (h c) x y -> b h c (x y)', h = self.heads)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        y = torch.einsum('b h d e, b h d n -> b h e n', context, q * self.scale)\n",
    "        y = self.conv2(rearrange(y, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w))\n",
    "        return rearrange(y, '(b f) c h w -> b c f h w', b = b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinopsToAndFrom(nn.Module):\n",
    "    def __init__(self, from_einops, to_einops, fn):\n",
    "        super().__init__()\n",
    "        self.from_einops = from_einops\n",
    "        self.to_einops = to_einops\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        shape = x.shape\n",
    "        reconstitute_kwargs = dict(tuple(zip(self.from_einops.split(' '), shape)))\n",
    "        x = self.fn(rearrange(x, f'{self.from_einops} -> {self.to_einops}'), **kwargs)\n",
    "        return rearrange(x, f'{self.to_einops} -> {self.from_einops}', **reconstitute_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32, rotary_emb = None):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.rotary_emb = rotary_emb\n",
    "        self.fc1 = torch.nn.Linear(dim, hidden_dim * 3, bias = False)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, dim, bias = False)\n",
    "\n",
    "    def forward(self, x, pos_bias = None, focus_present_mask = None):\n",
    "        n, device = x.shape[-2], x.device\n",
    "\n",
    "        qkv = self.fc1(x).chunk(3, dim = -1)\n",
    "\n",
    "        if (focus_present_mask is not None) and focus_present_mask.all():\n",
    "            values = qkv[-1]\n",
    "            return self.fc2(values)\n",
    "\n",
    "        q, k, v = rearrange_many(qkv, '... n (h d) -> ... h n d', h = self.heads)\n",
    "\n",
    "        if self.rotary_emb is not None:\n",
    "            q = self.rotary_emb.rotate_queries_or_keys(q * self.scale)\n",
    "            k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "        sim = einsum('... h i d, ... h j d -> ... h i j', q, k)\n",
    "\n",
    "        if pos_bias is not None:\n",
    "            sim = sim + pos_bias\n",
    "\n",
    "        if (focus_present_mask is not None) and not (~focus_present_mask).all():\n",
    "            attend_all_mask = torch.ones((n, n), device = device, dtype = torch.bool)\n",
    "            attend_self_mask = torch.eye(n, device = device, dtype = torch.bool)\n",
    "\n",
    "            mask = torch.where(\n",
    "                rearrange(focus_present_mask, 'b -> b 1 1 1 1'),\n",
    "                rearrange(attend_self_mask, 'i j -> 1 1 1 i j'),\n",
    "                rearrange(attend_all_mask, 'i j -> 1 1 1 i j'),\n",
    "            )\n",
    "\n",
    "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        y = einsum('... h i j, ... h j d -> ... h i d', attn, v)\n",
    "        return self.fc2(rearrange(y, '... h n d -> ... n (h d)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.has_cond = True\n",
    "        \n",
    "        dim = 64 # the width and height of the feature map will be 64\n",
    "        attn_heads = 8 # the number of heads for the spatial attention\n",
    "        attn_dim_head = 32 # the dimension of each head for the spatial attention\n",
    "\n",
    "        # temporal attention and its relative positional encoding\n",
    "        rotary_emb = RotaryEmbedding(attn_dim_head) # positional encoding for the temporal attention\n",
    "        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', 'b (h w) f c', Attention(dim, heads = attn_heads, dim_head = attn_dim_head, rotary_emb = rotary_emb))\n",
    "\n",
    "        self.time_rel_pos_bias = RelativePositionBias(heads = attn_heads, max_distance = 32)\n",
    "\n",
    "        # initial conv\n",
    "        init_kernel_size = 7\n",
    "        init_padding = init_kernel_size // 2\n",
    "        self.init_conv = nn.Conv3d(3, dim, (1, init_kernel_size, init_kernel_size), padding = (0, init_padding, init_padding))\n",
    "\n",
    "        self.init_temporal_attn = Residual(PreNorm(dim, temporal_attn(dim)))\n",
    "\n",
    "        # dimensions\n",
    "        dims = [dim, *map(lambda m: dim * m, [1, 2, 4, 8])]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time conditioning\n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # text conditioning\n",
    "        self.null_cond_emb = nn.Parameter(torch.randn(1, BERT_MODEL_DIM))\n",
    "        cond_dim = time_dim + BERT_MODEL_DIM\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        # block type\n",
    "        block_klass = partial(ResnetBlock, groups=8)\n",
    "        block_klass_cond = partial(block_klass, time=cond_dim)\n",
    "\n",
    "        # modules for all layers\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass_cond(dim_in, dim_out),\n",
    "                block_klass_cond(dim_out, dim_out),\n",
    "                Residual(PreNorm(dim_out, SpatialLinearAttention(dim_out, heads = attn_heads))),\n",
    "                Residual(PreNorm(dim_out, temporal_attn(dim_out))),\n",
    "                Downsample(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass_cond(mid_dim, mid_dim)\n",
    "\n",
    "        spatial_attn = EinopsToAndFrom('b c f h w', 'b f (h w) c', Attention(mid_dim, heads = attn_heads))\n",
    "\n",
    "        self.mid_spatial_attn = Residual(PreNorm(mid_dim, spatial_attn))\n",
    "        self.mid_temporal_attn = Residual(PreNorm(mid_dim, temporal_attn(mid_dim)))\n",
    "\n",
    "        self.mid_block2 = block_klass_cond(mid_dim, mid_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_klass_cond(dim_out * 2, dim_in),\n",
    "                block_klass_cond(dim_in, dim_in),\n",
    "                Residual(PreNorm(dim_in, SpatialLinearAttention(dim_in, heads = attn_heads))),\n",
    "                Residual(PreNorm(dim_in, temporal_attn(dim_in))),\n",
    "                Upsample(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        out_dim = 3\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim * 2, dim),\n",
    "            nn.Conv3d(dim, out_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        *args,\n",
    "        cond_scale = 2.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        logits = self.forward(*args, null_cond_prob = 0., **kwargs)\n",
    "        if cond_scale == 1:\n",
    "            return logits\n",
    "\n",
    "        null_logits = self.forward(*args, null_cond_prob = 1., **kwargs)\n",
    "        return null_logits + (logits - null_logits) * cond_scale\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        time,\n",
    "        cond,\n",
    "        null_cond_prob = 0.,\n",
    "    ):\n",
    "        batch, device = x.shape[0], x.device\n",
    "\n",
    "        focus_present_mask = (lambda: prob_mask_like((batch,), 0, device = device))()\n",
    "\n",
    "        time_rel_pos_bias = self.time_rel_pos_bias(x.shape[2], device = x.device)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        x = self.init_temporal_attn(x, pos_bias = time_rel_pos_bias)\n",
    "\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time) if self.time_mlp is not None else None\n",
    "\n",
    "        # classifier free guidance\n",
    "        if self.has_cond:\n",
    "            batch, device = x.shape[0], x.device\n",
    "            mask = prob_mask_like((batch,), null_cond_prob, device = device)\n",
    "            cond = torch.where(rearrange(mask, 'b -> b 1'), self.null_cond_emb, cond)\n",
    "            t = torch.cat((t, cond), dim = -1)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, spatial_attn, temporal_attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = spatial_attn(x)\n",
    "            x = temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_spatial_attn(x)\n",
    "        x = self.mid_temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, spatial_attn, temporal_attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = spatial_attn(x)\n",
    "            x = temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(self, denoise, *, text_user_bert_cls = False, channels = 3, num_frames, image_size, timesteps = 1000, loss_type = 'l1', use_dynamic_thres = False, dynamic_thres_percentile = 0.9):\n",
    "        super().__init__()\n",
    "        self.denoise = denoise\n",
    "        self.channels = channels\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.timesteps = timesteps\n",
    "        self.loss_type = loss_type\n",
    "        self.use_dynamic_thres = use_dynamic_thres\n",
    "        self.dynamic_thres_percentile = dynamic_thres_percentile\n",
    "        self.text_user_bert_cls = text_user_bert_cls\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        steps = timesteps + 1\n",
    "        x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "        alphaprod_cum = torch.cos(((x - timesteps) + 0.008) / (1 + 0.008) * torch.pi / 2) ** 2\n",
    "        alphaprod_cum = alphaprod_cum / alphaprod_cum[0]\n",
    "        # Calculate betas\n",
    "        betas = torch.clip(1 - (alphaprod_cum[1:] / alphaprod_cum[:-1]), 0, 0.9999)\n",
    "        alphas = 1 - betas\n",
    "        alphaprod_cum = torch.cumprod(alphas, axis=0)\n",
    "        \n",
    "        # Calculate previous alphas products\n",
    "        prev_alphaprod_cum = F.pad(alphaprod_cum[:-1], (1, 0), value = 1)\n",
    "\n",
    "        timesteps = betas.shape[0]\n",
    "\n",
    "        self.num_steps = int(timesteps)\n",
    "\n",
    "        # Write \"register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\" in a different way\n",
    "        def register_buffer(name, val):\n",
    "            self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', torch.cumprod(alphas, dim = 0))\n",
    "        register_buffer('alphas_cumprod_prev', prev_alphaprod_cum)\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphaprod_cum))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1 / alphaprod_cum))\n",
    "        register_buffer('sqrt_alphas_cumprod_minus_one', torch.sqrt(alphaprod_cum - 1))\n",
    "        # register buffer for 1 - alpha_cumprod and log of it\n",
    "        register_buffer('one_minus_alphas_cumprod', 1 - alphaprod_cum)\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1 - alphaprod_cum))\n",
    "        # register buffer for recip of 1 - alpha_cumprod\n",
    "        register_buffer('recip_one_minus_alphas_cumprod', 1 / (1 - alphaprod_cum))\n",
    "\n",
    "        posterior_var = betas * (1. - prev_alphaprod_cum) / (1. - alphaprod_cum)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_var)\n",
    "\n",
    "        # Since posterior variance is 0 for the first time step, we need to clamp it for numerical stability\n",
    "        register_buffer('posterior_variance_clipped', torch.clamp(posterior_var, min = 1e-20))\n",
    "        register_buffer('log_posterior_variance_clipped', torch.log(torch.clamp(posterior_var, min = 1e-20)))\n",
    "\n",
    "        # calculate posterior mean coefficients\n",
    "        mean_coef1 = betas * torch.sqrt(prev_alphaprod_cum) / (1 - alphaprod_cum)\n",
    "        mean_coef2 = (1 - prev_alphaprod_cum) * torch.sqrt(alphaprod_cum) / (1 - alphaprod_cum)\n",
    "\n",
    "        register_buffer('posterior_mean_coef1', mean_coef1)\n",
    "        register_buffer('posterior_mean_coef2', mean_coef2)\n",
    "\n",
    "        self.text_user_bert_cls = text_user_bert_cls\n",
    "        self.dynamic_thres_percentile = dynamic_thres_percentile\n",
    "        self.use_dynamic_thres = use_dynamic_thres\n",
    "        \n",
    "    def sample_q(self, x, t, noise=None, *args, **kwargs):\n",
    "        if noise is not None:\n",
    "            noise = noise\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x.shape) * x +\n",
    "            extract(self.sqrt_alphas_cumprod_minus_one, t, x.shape) * noise\n",
    "        )\n",
    "        \n",
    "    def p_losses(self, x, t, cond=None, noise=None, *args, **kwargs):\n",
    "        if noise is not None:\n",
    "            noise = noise\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "\n",
    "        b, c, f, h, w, device = *x.shape, x.device\n",
    "\n",
    "        noisy_x = self.sample_q(x=x, t=t, noise=noise)\n",
    "\n",
    "        if isinstance(cond, (list, tuple)) and all(isinstance(s, str) for s in cond):\n",
    "            cond = bert_embed(tokenize(cond), return_cls_repr = self.text_user_bert_cls)\n",
    "            cond = cond.to(device)\n",
    "\n",
    "        output_renoise = self.denoise(noisy_x, t, cond=cond, *args, **kwargs)\n",
    "\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = F.l1_loss(noise, output_renoise)\n",
    "        elif self.loss_type == 'l2':\n",
    "            loss = F.mse_loss(noise, output_renoise)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        b, image_size, device = x.shape[0], self.image_size, x.device\n",
    "        check_shape(x, 'b c f h w', c = self.channels, f = self.num_frames, h = image_size, w = image_size)\n",
    "        return self.p_losses(x * 2 - 1, torch.randint(0, self.num_steps, (b,), device=device).long(), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=64,\n",
    "    num_frames=10,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type='l1',     # L1 or L2\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert = BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the model\n",
    "if os.path.exists('diffusion.pt'):\n",
    "    diffusion.load_state_dict(torch.load('diffusion.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[535], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     30\u001b[0m \u001b[39m# get the loss\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m loss \u001b[39m=\u001b[39m diffusion(gifs, cond\u001b[39m=\u001b[39;49mconds)\n\u001b[0;32m     32\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     34\u001b[0m \u001b[39m# update the weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[529], line 107\u001b[0m, in \u001b[0;36mGaussianDiffusion.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m b, image_size, device \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size, x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    106\u001b[0m check_shape(x, \u001b[39m'\u001b[39m\u001b[39mb c f h w\u001b[39m\u001b[39m'\u001b[39m, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels, f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_frames, h \u001b[39m=\u001b[39m image_size, w \u001b[39m=\u001b[39m image_size)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_losses(x \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, torch\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_steps, (b,), device\u001b[39m=\u001b[39;49mdevice)\u001b[39m.\u001b[39;49mlong(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[529], line 92\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_losses\u001b[1;34m(self, x, t, cond, noise, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     cond \u001b[39m=\u001b[39m bert_embed(tokenize(cond), return_cls_repr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_user_bert_cls)\n\u001b[0;32m     90\u001b[0m     cond \u001b[39m=\u001b[39m cond\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 92\u001b[0m output_renoise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdenoise(noisy_x, t, cond\u001b[39m=\u001b[39;49mcond, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     95\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39ml1_loss(noise, output_renoise)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[528], line 114\u001b[0m, in \u001b[0;36mUnet3D.forward\u001b[1;34m(self, x, time, cond, null_cond_prob)\u001b[0m\n\u001b[0;32m    110\u001b[0m batch, device \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    112\u001b[0m focus_present_mask \u001b[39m=\u001b[39m (\u001b[39mlambda\u001b[39;00m: prob_mask_like((batch,), \u001b[39m0\u001b[39m, device \u001b[39m=\u001b[39m device))()\n\u001b[1;32m--> 114\u001b[0m time_rel_pos_bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_rel_pos_bias(x\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m], device \u001b[39m=\u001b[39;49m x\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    116\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_conv(x)\n\u001b[0;32m    118\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_temporal_attn(x, pos_bias \u001b[39m=\u001b[39m time_rel_pos_bias)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[518], line 17\u001b[0m, in \u001b[0;36mRelativePositionBias.forward\u001b[1;34m(self, n, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m rp_bucket \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     16\u001b[0m buckets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuckets \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m---> 17\u001b[0m rp_bucket \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (n \u001b[39m<\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mlong() \u001b[39m*\u001b[39m buckets\n\u001b[0;32m     18\u001b[0m n \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(\u001b[39m-\u001b[39mrel_pos)\n\u001b[0;32m     20\u001b[0m max_exact \u001b[39m=\u001b[39m buckets \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'long'"
     ]
    }
   ],
   "source": [
    "BATCH = 1\n",
    "optim = torch.optim.Adam(diffusion.parameters(), lr=1e-4)\n",
    "\n",
    "# train the diffusion model\n",
    "for i in range(20):\n",
    "    gif_names = os.listdir('gifs_64')\n",
    "    \n",
    "    # loop through the gifs 32 at a time\n",
    "    for j in range(0, len(gif_names), BATCH):\n",
    "        # the conds are the names of the gifs, remove the .gif extension\n",
    "        names = gif_names[j:j+BATCH]\n",
    "        conds = [name.split('.')[0] for name in names]\n",
    "        \n",
    "        # read the gifs and convert to tensors\n",
    "        gifs = []\n",
    "        for name in names:\n",
    "            gif = imageio.mimread('gifs_64/' + name)\n",
    "            if len(gif) < 10:\n",
    "                gif = np.concatenate([gif, np.repeat(gif[-1:], 10 - len(gif), axis=0)])\n",
    "            gif = gif[:10]\n",
    "            gif = torch.tensor(gif).cuda()\n",
    "            gif = gif.permute(3, 0, 1, 2) # (channels, frames, height, width)\n",
    "            gif = gif / 127.5 - 1\n",
    "            gifs.append(gif)\n",
    "        gifs = torch.stack(gifs)\n",
    "        \n",
    "        # zero the gradients\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # get the loss\n",
    "        loss = diffusion(gifs, cond=conds)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights\n",
    "        optim.step()\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            print(f'epoch {i}, batch {j}, loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(diffusion.state_dict(), 'diffusion.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a baby is playing with an orange toy\n"
     ]
    }
   ],
   "source": [
    "# get the 230'th name from gifs_64\n",
    "best = os.listdir('gifs_64')[230].split('.')[0]\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [1:01:20<00:00,  3.68s/it]   \n"
     ]
    }
   ],
   "source": [
    "txt = 'green'\n",
    "\n",
    "output_gif = diffusion.sample(cond=[txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tensor to a gif that we can see\n",
    "out = output_gif[0].cpu().numpy()\n",
    "out = np.transpose(out, (1, 2, 3, 0))\n",
    "# the output is between -1 and 1, so we need to scale it to 0-255\n",
    "out = ((out + 1) / 2 * 255).astype(np.uint8)\n",
    "imageio.mimsave('out.gif', out, duration = 1000, loop = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
